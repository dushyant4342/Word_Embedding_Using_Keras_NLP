# -*- coding: utf-8 -*-
"""Word Embedding Using Keras- NLP | Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1809nLiYxRKgKKFJQA3JHjqux4qK97AxC
"""

import tensorflow as tf     #for tensorflow version >2.0

tf.__version__

from tensorflow.keras.preprocessing.text import one_hot

sent=[  'the glass of milk',
     'the glass of juice',
     'the cup of tea',
    'I am a good boy',
     'I am a good developer',
     'understand the meaning of words',
     'your videos are good',]

sent

voc_size=10000

onehot_rep=[one_hot(words,voc_size)for words in sent]
onehot_rep                    #we will be getting an index for every word

from tensorflow.keras.layers import Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences

import numpy as np

sent_length=8
embedded_docs= pad_sequences(onehot_rep,padding='pre',maxlen=sent_length)
embedded_docs

dim=10

model=Sequential()
model.add(Embedding(voc_size,10,input_length=sent_length))
model.compile('adam','mse')

model.summary()

model.predict(embedded_docs)

embedded_docs[0]

model.predict(embedded_docs[0])  #0 is converted in 10 vectors(every converted in 10 vectors)

